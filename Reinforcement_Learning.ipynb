{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p4guiler4/Reinforcement_Learning/blob/master/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM0P-E3FY_J6",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "In a way, Reinforcement Learning is the science of making optimal decisions using experiences. Breaking it down, the process of Reinforcement Learning involves these simple steps:\n",
        "\n",
        "1. Observation of the environment\n",
        "2. Deciding how to act using some strategy\n",
        "3. Acting accordingly\n",
        "4. Receiving a reward or penalty\n",
        "5. Learning from the experiences and refining our strategy\n",
        "6. Iterate until an optimal strategy is found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k287cFSmYxyr",
        "colab_type": "text"
      },
      "source": [
        "![The Reinforcement Learning Process](https://www.learndatasci.com/documents/14/Reinforcement-Learning-Animation.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g90xP03FT29m",
        "colab_type": "code",
        "outputId": "0c231819-3f0d-4889-95cd-cf2f3f1edfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.4)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.15)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.6.16)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bltbROlJYinc",
        "colab_type": "text"
      },
      "source": [
        "# Gym's interface\n",
        "\n",
        "Gym provides different game environments which we can plug into our code and test an agent. The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.\n",
        "\n",
        "We'll be using the Gym environment called Taxi-V2, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzVFeWpgUQ7M",
        "colab_type": "code",
        "outputId": "acad668a-1423-414f-e10d-28cc6156c7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGuL7vNhYCjf",
        "colab_type": "text"
      },
      "source": [
        "The core gym interface is env, which is the unified environment interface. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRt6UGFX5qV",
        "colab_type": "text"
      },
      "source": [
        "# The Problem\n",
        "\n",
        "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IorfvXKKV3L9",
        "colab_type": "code",
        "outputId": "7f9e9987-65fa-4a74-af71-684f1f09760c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space)) # 6 actions: 4 directions + pickup + dropoff\n",
        "print(\"State Space {}\".format(env.observation_space)) # taxi + passenger + destination locations"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[43mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3gRwe5VXu9g",
        "colab_type": "text"
      },
      "source": [
        "* The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "\n",
        "* The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "\n",
        "* R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH4JYrnZaVCN",
        "colab_type": "text"
      },
      "source": [
        "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
        "\n",
        "So, our taxi environment has 5 (grid height, taxi Y) ×5 (grid width, taxi X) ×5 (passengers location, including inside taxi) ×4 (destination) = **500 total possible states.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHlIkXHlXhrr",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement Learning will learn a mapping of **states** to the optimal **action** to perform in that state by *exploration*, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.\n",
        "\n",
        "The optimal action for each state is the action that has the **highest cumulative long-term reward.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71P3rWr3bV8h",
        "colab_type": "code",
        "outputId": "4d641269-2bfd-4152-c2a6-24bedf6ff4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke2NG0rjcDBZ",
        "colab_type": "text"
      },
      "source": [
        "# The Reward Table\n",
        "\n",
        "When the Taxi environment is created, there is an initial **Reward table** that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states × actions matrix.\n",
        "\n",
        "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqFNI0sMcQJq",
        "colab_type": "code",
        "outputId": "035ad922-8f67-442f-c06d-8e327b1af880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lM5o1GxcvDl",
        "colab_type": "text"
      },
      "source": [
        "**This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.**\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "* The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
        "\n",
        "* In this env, probability is always 1.0.\n",
        "\n",
        "* The nextstate is the state we would be in if we take the action at this index of the dict\n",
        "\n",
        "* All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
        "\n",
        "* done is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
        "\n",
        "Note that if our agent chose to explore action three (3) in this state it would be going West into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the long-term reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgR_g1bjeMS2",
        "colab_type": "text"
      },
      "source": [
        "# Solving the environment with Brute Force\n",
        "\n",
        "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
        "\n",
        "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vWi2ADheWWh",
        "colab_type": "code",
        "outputId": "0df3565f-e01d-4cc1-c09d-508f8722052d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Ojo, la lógica del movimiento del taxi y de coger y soltar pasajeros está implementada en AIGym. En nuestro caso, estará implementada por escrituras y lecturas en el CHT_CLI.\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 651\n",
            "Penalties incurred: 212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAKhOVeAfRhK",
        "colab_type": "code",
        "outputId": "79751006-4fbc-4573-83a3-b1f872327337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 651\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcsEaryWgoqm",
        "colab_type": "text"
      },
      "source": [
        "Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
        "\n",
        "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCOkhAkMiNJC",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning (Quality Learning)\n",
        "\n",
        "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
        "\n",
        "In our Taxi environment, we have the *reward table, P*, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value **to remember if that action was beneficial**.\n",
        "\n",
        "The values store in the *Q-table* are called a Q-values, and they map to a **(state, action)** combination.\n",
        "\n",
        "A Q-value for a particular state-action combination **is representative of the \"quality\" of an action taken from that state**. Better Q-values imply better chances of getting greater rewards.\n",
        "\n",
        "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
        "\n",
        "Q-values are initialized to an arbitrary value, and **as the agent exposes itself to the environment and receives different rewards by executing different actions**, the Q-values are updated using the equation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COkU5zjckxE7",
        "colab_type": "text"
      },
      "source": [
        "![texto alternativo](https://wikimedia.org/api/rest_v1/media/math/render/svg/59db58edf1222b292e40706e503ed5974553606b)\n",
        "\n",
        "Where:\n",
        "\n",
        "- α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n",
        "\n",
        "- γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
        "\n",
        "γ = 0 (cigarra)\n",
        "γ  = 1 (hormiga)\n",
        "\n",
        "# What is this saying?\n",
        "\n",
        "We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state (from the Reward Table P), and the discounted maximum reward from the next state we will be in once we take the current action.\n",
        "\n",
        "Basically, we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state. This will eventually cause our taxi to consider the route with the best rewards strung together.\n",
        "\n",
        "The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). The way we store the Q-values for each state and action is through a Q-table.\n",
        "\n",
        "Entrenamiento:\n",
        "\n",
        "Q <- Q + (recompensa inmediata + recompensa futura si sigo esta estrategia)\n",
        "\n",
        "# Q-Table\n",
        "\n",
        "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training. Note that the Q-table has the same dimensions as the reward table, but it has a completely different purpose.\n",
        "\n",
        "![texto alternativo](https://upload.wikimedia.org/wikipedia/commons/e/e0/Q-Learning_Matrix_Initialized_and_After_Training.png)\n",
        "\n",
        "# Summing up the Q-Learning Process\n",
        "\n",
        "Breaking it down into steps, we get\n",
        "\n",
        "* Initialize the Q-table by all zeros.\n",
        "\n",
        "* Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
        "\n",
        "* Travel to the next state (S') as a result of that action (a).\n",
        "\n",
        "* For all possible actions from the state (S') select the one with the highest Q-value.\n",
        "\n",
        "* Update Q-table values using the equation.\n",
        "\n",
        "* Set the next state as the current state.\n",
        "\n",
        "* If goal state is reached, then end and repeat the process.\n",
        "\n",
        "# Exploiting learned values\n",
        "\n",
        "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
        "\n",
        "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called ϵ \"epsilon\" to cater to this during training.\n",
        "\n",
        "**Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further.** Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVDfPNcbrPQP",
        "colab_type": "text"
      },
      "source": [
        "# Training the Agent\n",
        "\n",
        "Initialize the Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OqSeVr9pM30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKAAjZkjrdmi",
        "colab_type": "text"
      },
      "source": [
        "We can now create the training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
        "\n",
        "In the first part of while not done, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the epsilon value and comparing it to the random.uniform(0, 1) function, which returns an arbitrary number between 0 and 1.\n",
        "\n",
        "We execute the chosen action in the environment to obtain the next_state and the reward from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the next_state, and with that, we can easily update our Q-value to the new_q_value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtoHR9o3rV64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space. Me muevo al azar por explorar\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values. Tomo la mejor estrategia según lo que aprendí\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) # Observo qué ha pasado por mi acción. He sido castigado o premiado y tengo un nuevo estado\n",
        "        \n",
        "            # Ojo, la lógica del movimiento del taxi y de coger y soltar pasajeros está implementada en AIGym. En nuestro caso, estará implementada por escrituras y lecturas en el CHT_CLI\n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # Fórmula. Calculo la calidad de esta estrategia a corto y medio plazo\n",
        "        q_table[state, action] = new_value # Aprendo la estrategia\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state # actualizo el estado (y por tanto el entorno)\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQ9Vi_nr_6-",
        "colab_type": "text"
      },
      "source": [
        "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPM12_QrWhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3aGcTgMr9w4",
        "colab_type": "text"
      },
      "source": [
        "The max Q-value is \"north\" (-1.971), so it looks like Q-learning has effectively learned the best action to take in our illustration's state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvyGDjssDnG",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the agent\n",
        "\n",
        "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKr_DsEBsHJO",
        "colab_type": "code",
        "outputId": "0def3aae-fb95-4af6-b25f-455618279210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "                    \n",
        "        # Put each rendered frame into dict for animation\n",
        "        \n",
        "        frames.append({\n",
        "         'frame': env.render(mode='ansi'),\n",
        "         'state': state,\n",
        "         'action': action,\n",
        "         'reward': reward,\n",
        "         'episode': ep\n",
        "         }\n",
        "        )\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.7\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isoEXLV59dv1",
        "colab_type": "code",
        "outputId": "c17d61dc-df2f-4b3a-fdba-0295c4759b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"EPISODE: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "EPISODE: 99\n",
            "Timestep: 1270\n",
            "State: 85\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAJkysMwsKuu",
        "colab_type": "text"
      },
      "source": [
        "We can see from the evaluation, the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
      ]
    }
  ]
}